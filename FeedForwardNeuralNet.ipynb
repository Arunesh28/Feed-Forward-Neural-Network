{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 13:20:08.376439: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 13:20:08.629979: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-14 13:20:08.630002: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-14 13:20:09.510164: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 13:20:09.510246: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 13:20:09.510255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import sklearn.model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion Mnist Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, Y), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = len(np.unique(Y))\n",
    "# for i in range(num_classes):\n",
    "unique_classes = [X[np.where(Y == i)[0][0]] for i in range(num_classes)]\n",
    "class_names = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\",\n",
    "               \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "for i in range(1, num_classes+1):\n",
    "    plt.subplot(2, 5, i)\n",
    "    plt.imshow(unique_classes[i-1], cmap=plt.get_cmap('gray'))\n",
    "    plt.title(class_names[i-1], fontdict={'fontsize': 7})\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training DataSet = 54000\n",
      "Size of Validation DataSet = 6000\n",
      "Size of Test DataSet = 10000\n",
      "Number of classes = 10\n",
      "Number of features = 784\n"
     ]
    }
   ],
   "source": [
    "num_features = np.shape(X)[1]*np.shape(X)[2]\n",
    "X = X.reshape(np.shape(X)[0], 784)\n",
    "X_test = X_test.reshape(np.shape(X_test)[0], 784)\n",
    "X_train, Xv, Y_train, Yv = sklearn.model_selection.train_test_split(\n",
    "    X, Y, test_size=0.1, random_state=4, shuffle=True)\n",
    "print(\"Size of Training DataSet =\", len(X_train))\n",
    "print(\"Size of Validation DataSet =\", len(Xv))\n",
    "print(\"Size of Test DataSet =\", len(X_test))\n",
    "print(\"Number of classes =\", num_classes)\n",
    "print(\"Number of features =\", num_features)\n",
    "X_train = X_train.T\n",
    "Xv = Xv.T\n",
    "X_test = X_test.T\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_layer_nodes = num_classes\n",
    "# input_layer_nodes = num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import softmax\n",
    "def sigmoid(x):\n",
    "    return 1. /(1. + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def der_tanh(x):\n",
    "    return 1-(np.tanh(x)**2)\n",
    "\n",
    "def der_relu(x):\n",
    "    return (x>0)*1\n",
    "\n",
    "def softmax(x):\n",
    "    return (np.exp(x)/np.sum(np.exp(x),axis = 0))\n",
    "\n",
    "def der_softmax(x):\n",
    "    return softmax(x) * (1-softmax(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hidden_layer:\n",
    "    '''\n",
    "        num_neurons : number of neurons in the layer\n",
    "        dim_in_layer : inlayer dimensions\n",
    "        dim_out_layer : outLayer dimensions\n",
    "        W : Weights\n",
    "        b : Biases\n",
    "        grad_W : gradient of weights\n",
    "        grad_B : gradient of biases\n",
    "    '''\n",
    "    def __init__(self,num_neurons,dim_in_layer,dim_out_layer = -1):\n",
    "        self.weight_initializers = {\"random\": self.random_initialization, \"Xavier\": self.xavier_intialization}\n",
    "        self.num_neurons = num_neurons\n",
    "        self.dim_in_layer = dim_in_layer\n",
    "        self.dim_out_layer = dim_out_layer\n",
    "        \n",
    "    def random_initialization(self, in_layer, out_layer):\n",
    "        return np.random.randn(in_layer, out_layer)\n",
    "\n",
    "    def xavier_intialization(self, in_layer, out_layer):\n",
    "        # return np.random.normal(0, np.sqrt(2 / (in_layer + out_layer)), size=(in_layer, out_layer))\n",
    "        return np.random.randn(in_layer, out_layer)* np.sqrt(2 / (in_layer + out_layer))\n",
    "    \n",
    "    def initialize_layer(self,initializer):\n",
    "        N = self.num_neurons\n",
    "        self.W = self.weight_initializers[initializer](in_layer =N ,out_layer = self.dim_in_layer)\n",
    "        self.b = np.zeros(shape=(N,1))\n",
    "        \n",
    "        self.grad_W = self.weight_initializers[initializer](in_layer = N,out_layer = self.dim_in_layer)\n",
    "        self.grad_b = np.zeros(shape=(N,1))\n",
    "        \n",
    "        self.u_mgd_W = np.zeros(shape=(N,self.dim_in_layer))\n",
    "        self.u_mgd_b = np.zeros(shape=(N,1))\n",
    "        \n",
    "        self.m_W = np.zeros(shape=(N,self.dim_in_layer))\n",
    "        self.m_b = np.zeros(shape=(N,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    '''\n",
    "        weight_initializers : dictionary with random , xavier\n",
    "        activation_funtions : dictionary with sigmoid, tanh, relu\n",
    "        der_activation_functions : dictionary with derivatives of the above\n",
    "        optimizer_funtions : dictionary with sgd, momentum, nestrov, rmsprop, adam, nadam}\n",
    "        activation : string\n",
    "        opitmizer : string\n",
    "        learning_rate : int\n",
    "        batch_size : int\n",
    "        num_epochs : int\n",
    "        num_features : dimension of X\n",
    "        num_hidden_layers : int, number of hidden layers\n",
    "        output_layer_dim : int\n",
    "        hidden_layer_dims : np.array with num_neurons in all hidden layer \n",
    "        weight_Decay : \n",
    "        X_train : Training Data (n,d)\n",
    "        Y_train : Training Data (n,)\n",
    "        Xv : Validation Data (n,d)\n",
    "        Yv : Validation Data (n,)\n",
    "        hidden_layers : np.array of objects to class hidden_layer dimensions = num_hidden_layers\n",
    "        output_layer : object to hidden_layer class\n",
    "        beta : momentum\n",
    "        beta1 : adam\n",
    "        beta2 : adam\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 num_features,\n",
    "                 weight_initializer,\n",
    "                 num_hidden_layers,\n",
    "                 hidden_layer_dims,\n",
    "                 optimizer,\n",
    "                 learning_rate,\n",
    "                 activation,\n",
    "                 X_train,\n",
    "                 Y_train,\n",
    "                 Xv,\n",
    "                 Yv,\n",
    "                 weight_decay,\n",
    "                 output_layer_dim,\n",
    "                 batch_size,\n",
    "                 num_epochs,\n",
    "                 output_activation = softmax,\n",
    "                 der_output_activation = der_softmax,\n",
    "                 beta=0.9,\n",
    "                 epsilon=1e-6,\n",
    "                 beta1=0.9,\n",
    "                 beta2=0.9):\n",
    "        self.weight_initializer = weight_initializer\n",
    "        self.activation_functions = {\"sigmoid\": sigmoid, \"tanh\": tanh, \"ReLU\": relu}\n",
    "        self.der_activation_functions = {\"sigmoid\": der_sigmoid, \"tanh\": der_tanh, \"ReLU\": der_relu}\n",
    "        self.optimizer_functions = {\"sgd\": self.sgd, \"momentum\": self.momentum,\"nesterov\": self.nestrov, \"rmsprop\": self.rmsprop, \"adam\": self.adam, \"nadam\": self.nadam}\n",
    "        self.activation = self.activation_functions[activation]\n",
    "        self.optimizer = self.optimizer_functions[optimizer]\n",
    "        self.der_activation = self.der_activation_functions[activation]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_layer_dim = output_layer_dim\n",
    "        self.hidden_layer_dims = hidden_layer_dims\n",
    "        self.num_features = num_features\n",
    "        self.output_activation = output_activation\n",
    "        self.der_output_activation = der_output_activation\n",
    "        self.weight_decay = weight_decay\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.Xv = Xv\n",
    "        self.Yv = Yv\n",
    "        self.beta = beta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        # return self\n",
    "    \n",
    "    def sgd(self,X,Y):\n",
    "        # self.ForwardPropagation(X)\n",
    "        # self.BackPropagation(self.output_layer.h,Y,X)\n",
    "        # L = self.hidden_layers\n",
    "        # eta = self.learning_rate\n",
    "        # for i in range(self.num_hidden_layers):\n",
    "        #     L[i].W = L[i].W - eta*L[i].grad_W\n",
    "        #     L[i].b = L[i].b - eta*L[i].grad_b\n",
    "        # self.output_layer.W = self.output_layer.W - eta*self.output_layer.grad_W\n",
    "        # self.output_layer.b = self.output_layer.b - eta*self.output_layer.grad_b\n",
    "        eta = self.learning_rate\n",
    "        W,b = self.getWB() # W,b are lists\n",
    "        N = len(W)\n",
    "        A,H = self.ForwardPropagation_new(X,W,b)\n",
    "        grad_w,grad_b,W = self.BackPropagation_new(H[N-1],Y,X,W,b,A,H)\n",
    "        for i in range(N):\n",
    "            W[i] = W[i] - eta*grad_w[i]\n",
    "            b[i] = b[i] - eta*grad_b[i]\n",
    "        self.putWB(W,b)\n",
    "        \n",
    "    def nestrov(self,X,Y):\n",
    "        L = self.hidden_layers\n",
    "        beta  = self.beta\n",
    "        eta = self.learning_rate\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            L[i].lookahead_W = L[i].W - beta*L[i].u_mgd_W\n",
    "            L[i].lookahead_b = L[i].b - beta*L[i].u_mgd_b\n",
    "        self.output_layer.lookahead_W = self.output_layer.W - beta*self.output_layer.u_mgd_W\n",
    "        self.output_layer.lookahead_b = self.output_layer.b - beta*self.output_layer.u_mgd_b\n",
    "        \n",
    "        self.ForwardPropagation(X)\n",
    "        self.BackPropagation(self.output_layer.h,Y,X)\n",
    "        \n",
    "        for i in range(self.num_hidden_layers):\n",
    "            L[i].u_mgd_W = beta*L[i].u_mgd_W + L[i].grad_W\n",
    "            L[i].u_mgd_b = beta*L[i].u_mgd_b + L[i].grad_b\n",
    "            L[i].W = L[i].W - eta*L[i].u_mgd_W\n",
    "            L[i].b = L[i].b - eta*L[i].u_mgd_b\n",
    "        self.output_layer.u_mgd_W = beta*self.output_layer.u_mgd_W + self.output_layer.grad_W\n",
    "        self.output_layer.u_mgd_b = beta*self.output_layer.u_mgd_b + self.output_layer.grad_b\n",
    "        self.output_layer.W = self.output_layer.W - eta*self.output_layer.u_mgd_W\n",
    "        self.output_layer.b = self.output_layer.b - eta*self.output_layer.u_mgd_b\n",
    "        # pass\n",
    "    \n",
    "    def momentum(self,X,Y):\n",
    "        self.ForwardPropagation(X)\n",
    "        self.BackPropagation(self.output_layer.h,Y,X)\n",
    "        L = self.hidden_layers\n",
    "        beta = self.beta\n",
    "        eta = self.learning_rate\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            L[i].u_mgd_W = beta*L[i].u_mgd_W + L[i].grad_W\n",
    "            L[i].u_mgd_b = beta*L[i].u_mgd_b + L[i].grad_b\n",
    "            L[i].W = L[i].W - eta*L[i].u_mgd_W\n",
    "            L[i].b = L[i].b - eta*L[i].u_mgd_b\n",
    "        self.output_layer.u_mgd_W = beta*self.output_layer.u_mgd_W + self.output_layer.grad_W\n",
    "        self.output_layer.u_mgd_b = beta*self.output_layer.u_mgd_b + self.output_layer.grad_b\n",
    "        self.output_layer.W = self.output_layer.W - eta*self.output_layer.u_mgd_W\n",
    "        self.output_layer.b = self.output_layer.b - eta*self.output_layer.u_mgd_b\n",
    "        # pass\n",
    "    \n",
    "    def rmsprop(self,X,Y):\n",
    "        # self.print_all()\n",
    "        \n",
    "        self.ForwardPropagation(X)\n",
    "        self.BackPropagation(self.output_layer.h,Y,X)\n",
    "        # print(self.output_layer.h)\n",
    "        L = self.hidden_layers\n",
    "        beta = self.beta\n",
    "        eta = self.learning_rate\n",
    "        epsilon = self.epsilon\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            L[i].u_mgd_W = beta*L[i].u_mgd_W + (1-beta)*L[i].grad_W\n",
    "            L[i].u_mgd_b = beta*L[i].u_mgd_b + (1-beta)*L[i].grad_b\n",
    "            L[i].W = L[i].W - (eta*L[i].grad_W/(np.sqrt(L[i].u_mgd_W)+epsilon))\n",
    "            L[i].b = L[i].b - (eta*L[i].grad_b/(np.sqrt(L[i].u_mgd_b)+epsilon))\n",
    "        self.output_layer.u_mgd_W = beta*self.output_layer.u_mgd_W + (1-beta)*self.output_layer.grad_W\n",
    "        self.output_layer.u_mgd_b = beta*self.output_layer.u_mgd_b + (1-beta)*self.output_layer.grad_b\n",
    "        self.output_layer.W = self.output_layer.W - (eta*self.output_layer.grad_W/(np.sqrt(self.output_layer.u_mgd_W)+epsilon))\n",
    "        self.output_layer.b = self.output_layer.b - (eta*self.output_layer.grad_b/(np.sqrt(self.output_layer.u_mgd_b)+epsilon))\n",
    "        \n",
    "        # self.print_all()\n",
    "        # pass\n",
    "    \n",
    "    def adam(self,X,Y):\n",
    "        self.ForwardPropagation(X)\n",
    "        self.BackPropagation(self.output_layer.h,Y,X)\n",
    "        L = self.hidden_layers\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        output = self.output_layer\n",
    "        eta = self.learning_rate\n",
    "        epsilon = self.epsilon\n",
    "        for i in range (self.num_hidden_layers):\n",
    "            L[i].m_W = beta1*L[i].m_W + (1-beta1)*L[i].grad_W\n",
    "            L[i].m_b = beta1*L[i].m_b + (1-beta1)*L[i].grad_b\n",
    "            \n",
    "            L[i].u_mgd_W = beta2*L[i].u_mgd_W + (1-beta2)*(L[i].grad_W**2)\n",
    "            L[i].u_mgd_b = beta2*L[i].u_mgd_b + (1-beta2)*(L[i].grad_b**2)\n",
    "            \n",
    "            m_hat_W = L[i].m_W/(1 - np.power(beta1,i+1))\n",
    "            m_hat_b = L[i].m_b/(1 - np.power(beta1,i+1))\n",
    "            u_hat_W = L[i].u_mgd_W/(1 - np.power(beta2,i+1))\n",
    "            u_hat_b = L[i].u_mgd_b/(1 - np.power(beta2,i+1))\n",
    "            \n",
    "            L[i].W = L[i].W - (eta*m_hat_W/(np.sqrt(u_hat_W)+epsilon))\n",
    "            L[i].b = L[i].b - (eta*m_hat_b/(np.sqrt(u_hat_b)+epsilon))\n",
    "        \n",
    "        output.m_W = beta1*output.m_W + (1-beta1)*output.grad_W\n",
    "        output.m_b = beta1*output.m_b + (1-beta1)*output.grad_b\n",
    "        \n",
    "        output.u_mgd_W = beta2*output.u_mgd_W + (1-beta2)*(output.grad_W**2)\n",
    "        output.u_mgd_b = beta2*output.u_mgd_b + (1-beta2)*(output.grad_b**2)\n",
    "        \n",
    "        m_hat_W = output.m_W/(1 - np.power(beta1,i+1))\n",
    "        m_hat_b = output.m_b/(1 - np.power(beta1,i+1))\n",
    "        u_hat_W = output.u_mgd_W/(1 - np.power(beta2,i+1))\n",
    "        u_hat_b = output.u_mgd_b/(1 - np.power(beta2,i+1))\n",
    "        \n",
    "        output.W = output.W - (eta*m_hat_W/(np.sqrt(u_hat_W)+epsilon))\n",
    "        output.b = output.b - (eta*m_hat_b/(np.sqrt(u_hat_b)+epsilon))\n",
    "            \n",
    "        # pass\n",
    "    def nadam(self,X,Y):\n",
    "        self.ForwardPropagation(X)\n",
    "        self.BackPropagation(self.output_layer.h,Y,X)\n",
    "        L = self.hidden_layers\n",
    "        beta1 = self.beta1\n",
    "        beta2 = self.beta2\n",
    "        output = self.output_layer\n",
    "        eta = self.learning_rate\n",
    "        epsilon = self.epsilon\n",
    "        for i in range (self.num_hidden_layers):\n",
    "            L[i].m_W = beta1*L[i].m_W + (1-beta1)*L[i].grad_W\n",
    "            L[i].m_b = beta1*L[i].m_b + (1-beta1)*L[i].grad_b\n",
    "            #hereeeeeeeee\n",
    "            L[i].u_mgd_W = beta2*L[i].u_mgd_W + (1-beta2)*(L[i].grad_W**2)\n",
    "            L[i].u_mgd_b = beta2*L[i].u_mgd_b + (1-beta2)*(L[i].grad_b**2)\n",
    "            #hereeeeeeeee\n",
    "            m_hat_W = L[i].m_W/(1 - np.power(beta1,i+1))\n",
    "            m_hat_b = L[i].m_b/(1 - np.power(beta1,i+1))\n",
    "            u_hat_W = L[i].u_mgd_W/(1 - np.power(beta2,i+1))\n",
    "            u_hat_b = L[i].u_mgd_b/(1 - np.power(beta2,i+1))\n",
    "            \n",
    "            L[i].W = L[i].W - (eta/(np.sqrt(u_hat_W+epsilon)))*(beta1*m_hat_W+(1-beta1)*L[i].grad_W/(1-beta1**(i+1)))\n",
    "            L[i].b = L[i].b - (eta/(np.sqrt(u_hat_b+epsilon)))*(beta1*m_hat_b+(1-beta1)*L[i].grad_b/(1-beta1**(i+1)))\n",
    "            # (eta/np.sqrt(v_w_hat+eps))* (beta1*m_w_hat+(1-beta1)*dw/(1-beta1**(i+1)))\n",
    "        \n",
    "        output.m_W = beta1*output.m_W + (1-beta1)*output.grad_W\n",
    "        output.m_b = beta1*output.m_b + (1-beta1)*output.grad_b\n",
    "        \n",
    "        output.u_mgd_W = beta2*output.u_mgd_W + (1-beta2)*(output.grad_W**2)\n",
    "        output.u_mgd_b = beta2*output.u_mgd_b + (1-beta2)*(output.grad_b**2)\n",
    "        \n",
    "        m_hat_W = output.m_W/(1 - np.power(beta1,i+1))\n",
    "        m_hat_b = output.m_b/(1 - np.power(beta1,i+1))\n",
    "        u_hat_W = output.u_mgd_W/(1 - np.power(beta2,i+1))\n",
    "        u_hat_b = output.u_mgd_b/(1 - np.power(beta2,i+1))\n",
    "        \n",
    "        output.W = output.W - (eta/(np.sqrt(u_hat_W+epsilon)))*(beta1*m_hat_W+(1-beta1)*output.grad_W/(1-beta1**(i+1)))\n",
    "        output.b = output.b - (eta/(np.sqrt(u_hat_b+epsilon)))*(beta1*m_hat_b+(1-beta1)*output.grad_b/(1-beta1**(i+1)))\n",
    "        # pass\n",
    "    \n",
    "    def Square_Error_Loss(self,Y_pred,Y_actual):\n",
    "        return np.mean((Y_pred-Y_actual)**2)\n",
    "    \n",
    "    def Cross_Entropy_Loss(self,Y_pred,Y_actual):\n",
    "        return -np.sum(Y_actual*np.log(Y_pred))/float(Y_pred.shape[0])\n",
    "    \n",
    "    def initialize_hidden_layers(self):\n",
    "        L = []\n",
    "        in_layer, out_layer = num_features,self.hidden_layer_dims[1]\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            if  i == 0:\n",
    "                in_layer = num_features\n",
    "            else:\n",
    "                in_layer = self.hidden_layer_dims[i-1]\n",
    "                \n",
    "            if i == self.num_hidden_layers-1:\n",
    "                out_layer = self.output_layer_dim\n",
    "            else:\n",
    "                out_layer = self.hidden_layer_dims[i+1]\n",
    "                \n",
    "            L.append(hidden_layer(self.hidden_layer_dims[i],in_layer,out_layer))\n",
    "            \n",
    "        self.hidden_layers = L\n",
    "        self.output_layer = hidden_layer(self.output_layer_dim,self.hidden_layer_dims[self.num_hidden_layers-1],-1)\n",
    "        \n",
    "    def sanitize_Y(self):\n",
    "            \n",
    "        temp = np.zeros((num_classes,self.Y_train.shape[0]))\n",
    "        for i in range(self.Y_train.shape[0]) :\n",
    "            temp[int(self.Y_train[i])][i] = 1\n",
    "        self.Y_train = temp\n",
    "        # self.Y_train = np.array(temp).T\n",
    "        \n",
    "        temp = np.zeros((num_classes,self.Yv.shape[0]))\n",
    "        for i in range(self.Yv.shape[0]) :\n",
    "            temp[int(self.Yv[i])][i] = 1\n",
    "        self.Yv = temp\n",
    "        \n",
    "        # temp = []\n",
    "        # for i in self.Yv :\n",
    "        #     temp1 = np.zeros(num_classes)\n",
    "        #     temp1[i]=1\n",
    "        #     temp.append(temp1)\n",
    "        # self.Yv = np.array(temp).T\n",
    "        # print(Yv[0])\n",
    "        # pass\n",
    "    \n",
    "    def initialize_NeuralNet(self):\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.hidden_layers[i].initialize_layer(self.weight_initializer)\n",
    "        self.output_layer.initialize_layer(self.weight_initializer)\n",
    "        self.sanitize_Y()\n",
    "        # pass\n",
    "    \n",
    "    def add_hidden(self,num_neurons):\n",
    "        self.hidden_layer_dims.append(num_neurons)\n",
    "        self.num_hidden_layers += 1\n",
    "        # pass\n",
    "    \n",
    "    def getWB(self):\n",
    "        weights = [x.W.copy() for x in self.hidden_layers]\n",
    "        biases =  [x.b.copy() for x in self.hidden_layers]\n",
    "        weights.append((self.output_layer.W).copy())\n",
    "        biases.append((self.output_layer.b).copy())\n",
    "        return weights,biases\n",
    "    \n",
    "    def putWB(self,W,b):\n",
    "        for i in range(len(W)-1):\n",
    "            self.hidden_layers[i].W = W[i]\n",
    "            self.hidden_layers[i].b = b[i]\n",
    "        self.output_layer.W = W[len(W)-1]\n",
    "        self.output_layer.b = b[len(W)-1]\n",
    "            \n",
    "    \n",
    "    def ForwardPropagation_new(self,X,W,b):\n",
    "        A = [None]*len(W)\n",
    "        H = [None]*len(b)\n",
    "        # print(len(W))\n",
    "        for i in range(len(W)):\n",
    "            if i == 0:\n",
    "                A[i] = b[i] + W[i]@X\n",
    "            else:\n",
    "                A[i] = b[i] + W[i]@H[i-1]\n",
    "            if i == len(W) - 1:\n",
    "                H[i] = self.output_activation(A[i])\n",
    "            else:\n",
    "                H[i] = self.activation(A[i])\n",
    "        return A,H\n",
    "    \n",
    "    \n",
    "    def ForwardPropagation(self,X):\n",
    "        L = self.hidden_layers\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            if i==0:\n",
    "                L[i].a = (L[i].b + np.matmul(L[i].W,X))\n",
    "            else:\n",
    "                L[i].a = (L[i].b + np.matmul(L[i].W,L[i-1].h))\n",
    "            L[i].h = self.activation(L[i].a)\n",
    "        self.output_layer.a = (self.output_layer.b + np.matmul(self.output_layer.W,L[self.num_hidden_layers-1].h))\n",
    "        # print(\"Output before activagtion\", self.output_layer.a)\n",
    "        # print(self.output_activation)\n",
    "        self.output_layer.h = self.output_activation(self.output_layer.a)\n",
    "        # print(\"Loss\", self.Cross_Entropy_Loss(self.output_layert))\n",
    "        # print(\"OUtput layer\", self.output_layer.h)\n",
    "        # pass\n",
    "    \n",
    "    def BackPropagation_new(self,Y_hat,Y,X,W,b,A,H):\n",
    "        grad_a = [None]*len(A)\n",
    "        grad_h = [None]*len(H)\n",
    "        grad_w = [None]*len(W)\n",
    "        grad_b = [None]*len(b)\n",
    "        N = len(W)\n",
    "        grad_a[N-1] = Y_hat - Y\n",
    "        \n",
    "        for i in range(N-1,-1,-1):\n",
    "            if i == 0:\n",
    "                grad_w[i] = grad_a[i]@X.T\n",
    "            else:\n",
    "                grad_w[i] = grad_a[i]@H[i-1].T\n",
    "            grad_b[i] = np.sum(grad_a[i],axis=1,keepdims=True)\n",
    "            if i>0 :\n",
    "                grad_h[i-1] = W[i].T@grad_a[i]\n",
    "                grad_a[i-1] = grad_h[i-1]*self.der_activation(A[i-1])\n",
    "                \n",
    "        for i in range(N):\n",
    "            W[i] += self.weight_decay*W[i]\n",
    "            \n",
    "        return grad_w,grad_b,W\n",
    "    \n",
    "    def BackPropagation(self,y_hat,y,X):\n",
    "        L = self.hidden_layers\n",
    "        N = self.num_hidden_layers\n",
    "        out_layer = self.output_layer\n",
    "        \n",
    "        out_layer.grad_a = y_hat - y\n",
    "        # out_layer.grad_W = (np.matmul(out_layer.grad_a,L[N-1].h.T)+self.weight_decay*out_layer.W)/self.batch_size\n",
    "        out_layer.grad_W = np.matmul(out_layer.grad_a,L[N-1].h.T)\n",
    "        # out_layer.grad_b = out_layer.grad_a\n",
    "        out_layer.grad_b = np.sum(out_layer.grad_a,axis=1,keepdims=True)#/self.batch_size\n",
    "        # print(\"b dims \",out_layer.grad_b.shape)\n",
    "        L[N-1].grad_h = np.matmul(out_layer.W.T,out_layer.grad_a)\n",
    "        L[N-1].grad_a = L[N-1].grad_h * self.der_activation(L[N-1].a)\n",
    "        \n",
    "        for i in range(N-1,-1,-1):\n",
    "            if(i==0):\n",
    "                # L[i].grad_W = (np.matmul(L[i].grad_a,X.T)+self.weight_decay*L[i].W)/self.batch_size\n",
    "                L[i].grad_W = np.matmul(L[i].grad_a,X.T)\n",
    "            else:\n",
    "                # L[i].grad_W = (np.matmul(L[i].grad_a,L[i-1].h.T)+self.weight_decay*L[i].W)/self.batch_size\n",
    "                L[i].grad_W = np.matmul(L[i].grad_a,L[i-1].h.T)\n",
    "            L[i].grad_b = np.sum(L[i].grad_a,axis=1,keepdims=True)#/self.batch_size\n",
    "            # L[i].grad_b = L[i].grad_a\n",
    "            if i>0:\n",
    "                L[i-1].grad_h = np.matmul(L[i].W.T,L[i].grad_a)\n",
    "                L[i-1].grad_a = L[i-1].grad_h * self.der_activation(L[i-1].a)\n",
    "        \n",
    "        # out_layer.W += self.weight_decay*out_layer.W\n",
    "        # for i in range(N):\n",
    "        #     L[i].W += self.weight_decay*L[i].W\n",
    "        # pass\n",
    "        \n",
    "    \n",
    "    def fit_NeuralNet(self):\n",
    "        self.initialize_hidden_layers()\n",
    "        self.initialize_NeuralNet()\n",
    "        for curr_epoch in range(self.num_epochs):\n",
    "            print(curr_epoch)\n",
    "            for i in range(0,self.X_train.shape[1],self.batch_size):\n",
    "                curr_batch = min(self.X_train.shape[1]-i,self.batch_size)\n",
    "                # self.ForwardPropagation(self.X_train[:,i:i+curr_batch])\n",
    "                # self.BackPropagation(self.output_layer.h,self.Y_train[:,i:i+curr_batch],self.X_train[:,i:i+curr_batch])\n",
    "                self.optimizer(self.X_train[:,i:i+curr_batch],self.Y_train[:,i:i+curr_batch])\n",
    "                \n",
    "                \n",
    "            # Y_pred = self.predict_NeuralNet(X_train)\n",
    "            # print(\"Loss \", self.Cross_Entropy_Loss(Y_pred, Y_train))\n",
    "        # pass\n",
    "    \n",
    "    def predict_NeuralNet(self,X):\n",
    "        self.ForwardPropagation(X)\n",
    "        # for i in range(10):\n",
    "        #     print(self.output_layer.h[0][i], sep=\" \")\n",
    "        Y_pred = np.argmax(self.output_layer.h,axis=0)\n",
    "        return Y_pred\n",
    "        \n",
    "    def print_all(self):\n",
    "        # print(\"weight_initializer = \",self.weight_initializer)\n",
    "        # print(\"activation_functions = \",self.activation_functions)\n",
    "        # print(\"optimizer_functions = \",self.optimizer_functions)\n",
    "        # print(\"weight_initializers = \",self.weight_initializers)\n",
    "        # print(\"activation = \",self.activation)\n",
    "        # print(\"optimizer = \",self.optimizer)\n",
    "        # print(\"learning_rate = \",self.learning_rate)\n",
    "        # print(\"batch_size = \",self.batch_size)\n",
    "        # print(\"num_epochs = \",self.num_epochs)\n",
    "        # print(\"num_hidden_layers = \",self.num_hidden_layers)\n",
    "        # print(\"output_layer_dim = \",self.output_layer_dim)\n",
    "        # print(\"hidden_layer_dims = \",self.hidden_layer_dims)\n",
    "        # print(\"num_features = \",self.num_features)\n",
    "        # print(\"weight_Decay = \",self.weight_Decay)\n",
    "        # print(\"X_train = \",self.X_train)\n",
    "        # print(\"Y_train = \",self.Y_train)\n",
    "        # print(\"Xv = \",self.Xv)\n",
    "        # print(\"Yv = \",self.Yv)\n",
    "        # print(self.output_layer.h)\n",
    "        # print(self.num_hidden_layers)\n",
    "        \n",
    "        L = self.hidden_layers\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            \n",
    "            # print(i)\n",
    "            # print(i,\" \",self.hidden_layers[i].dim_in_layer,self.hidden_layers[i].num_neurons,self.hidden_layers[i].dim_out_layer,self.hidden_layers[i].W.shape,self.hidden_layers[i].b.shape)\n",
    "            # print(self.hidden_layers[i].W)\n",
    "            # print(self.hidden_layers[i].b)\n",
    "            # print(L[i].W.shape,L[i].grad_W.shape,L[i].u_mgd_W.shape,L[i].m_W.shape)\n",
    "            # print(L[i].grad_a.shape)\n",
    "            print(L[i].W)\n",
    "            print(L[i].b)\n",
    "            print(L[i].grad_W)\n",
    "            print(L[i].grad_b)\n",
    "        print(self.output_layer.W)\n",
    "        print(self.output_layer.b)\n",
    "        print(self.output_layer.grad_W)\n",
    "        print(self.output_layer.grad_b)\n",
    "        \n",
    "        print()\n",
    "        # print(self.output_layer.W.shape,self.output_layer.grad_W.shape)\n",
    "        # print(self.output_layer.grad_a.shape)\n",
    "        # print(\"output-layer\")\n",
    "        # print(self.output_layer.W.shape)\n",
    "        # print(\"o  \",self.output_layer.num_neurons,self.output_layer.dim_in_layer,self.output_layer.dim_out_layer)\n",
    "        # pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ty \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m])\n\u001b[1;32m      3\u001b[0m re \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m re[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m ty\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(re)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# print(X_train.shape)\n",
    "ty = np.array([1,2,3])\n",
    "re = np.zeros(3)\n",
    "re[0] = ty\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4262/755351156.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. /(1. + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.26272611e-03  8.71207906e-01  2.33430945e+00 ... -2.75422589e-01\n",
      "  -1.01124208e+00  4.57771220e-01]\n",
      " [ 2.78425651e+00  1.48106403e-01  9.25187542e-01 ...  6.91842700e-01\n",
      "   9.67828608e-01  1.35030080e-01]\n",
      " [ 5.54253350e-01  1.07022227e+00  1.35703374e+00 ...  1.63997127e+00\n",
      "  -7.82557817e-01 -1.28631346e+00]\n",
      " ...\n",
      " [ 4.37681624e-01 -1.73838913e-01 -4.29534582e-01 ... -5.90345816e-01\n",
      "   6.33414762e-01  3.29527445e-01]\n",
      " [ 2.07025063e+00 -6.95266238e-01  5.11203451e-02 ... -9.93284092e-01\n",
      "  -8.12844365e-01  1.46376682e+00]\n",
      " [ 6.55522133e-02  8.28898714e-02 -7.73725976e-01 ...  1.32200472e+00\n",
      "  -9.97570774e-01 -1.84312039e-01]]\n",
      "[[ 6.93708747e-04]\n",
      " [-4.65906995e-04]\n",
      " [ 6.87403632e-04]\n",
      " [-6.49268647e-04]\n",
      " [ 3.98727293e-04]\n",
      " [-2.39848349e-04]\n",
      " [ 2.18384804e-04]\n",
      " [-3.07761908e-04]\n",
      " [ 5.02586538e-04]\n",
      " [-6.41869189e-04]\n",
      " [-2.51317235e-04]\n",
      " [ 7.45393000e-05]\n",
      " [ 4.08850768e-04]\n",
      " [-6.96924155e-04]\n",
      " [-7.60933987e-04]\n",
      " [ 3.03780067e-04]\n",
      " [ 6.82253990e-04]\n",
      " [ 7.88153274e-05]\n",
      " [-2.48497120e-04]\n",
      " [-6.31669503e-04]\n",
      " [ 2.84065558e-04]\n",
      " [-3.57294511e-04]\n",
      " [-3.46817529e-04]\n",
      " [-9.31209519e-04]\n",
      " [ 2.74575164e-04]\n",
      " [-4.83338064e-04]\n",
      " [-3.99463841e-04]\n",
      " [ 6.83454200e-04]\n",
      " [ 6.18861005e-04]\n",
      " [-1.01819442e-03]\n",
      " [ 7.75115070e-04]\n",
      " [ 9.39247387e-05]\n",
      " [ 1.38204492e-04]\n",
      " [-5.65934165e-04]\n",
      " [ 1.41341329e-05]\n",
      " [ 3.56031880e-04]\n",
      " [ 5.30825432e-04]\n",
      " [-9.59725901e-05]\n",
      " [-5.85684918e-04]\n",
      " [ 4.33241244e-04]\n",
      " [ 1.24562798e-05]\n",
      " [-2.36100385e-04]\n",
      " [-2.39131700e-04]\n",
      " [ 8.08243781e-04]\n",
      " [-1.04768847e-03]\n",
      " [-3.35080747e-04]\n",
      " [-3.11847291e-04]\n",
      " [-1.27035341e-03]\n",
      " [-6.97956287e-04]\n",
      " [-3.51355564e-04]\n",
      " [ 3.85223300e-05]\n",
      " [-5.32993928e-04]\n",
      " [-3.39005718e-04]\n",
      " [-6.23642727e-04]\n",
      " [-2.34207777e-04]\n",
      " [ 7.27485903e-05]\n",
      " [ 5.43906281e-04]\n",
      " [-8.73237547e-04]\n",
      " [-1.52682051e-04]\n",
      " [ 1.01330250e-03]\n",
      " [ 5.82174538e-04]\n",
      " [ 2.06881386e-04]\n",
      " [ 4.45323621e-04]\n",
      " [ 8.42428066e-04]\n",
      " [ 3.46359826e-04]\n",
      " [-7.91503697e-05]\n",
      " [ 3.72865873e-04]\n",
      " [-9.62093660e-04]\n",
      " [ 2.04566451e-04]\n",
      " [ 5.29897130e-05]\n",
      " [-1.57309681e-04]\n",
      " [ 3.78934674e-04]\n",
      " [ 2.63566271e-04]\n",
      " [-4.43977626e-04]\n",
      " [ 3.71847093e-04]\n",
      " [-4.66270639e-04]\n",
      " [ 2.04387479e-05]\n",
      " [ 3.22160916e-04]\n",
      " [ 7.29470467e-04]\n",
      " [-2.83290027e-04]\n",
      " [-3.84836954e-04]\n",
      " [-1.25435895e-04]\n",
      " [ 4.31556435e-04]\n",
      " [-6.47390633e-04]\n",
      " [ 5.39627098e-04]\n",
      " [ 7.25528485e-04]\n",
      " [ 7.35490529e-05]\n",
      " [-6.60887247e-04]\n",
      " [ 3.65303661e-04]\n",
      " [-4.98513339e-05]\n",
      " [-8.86511621e-04]\n",
      " [ 1.15678936e-03]\n",
      " [ 4.76915078e-04]\n",
      " [ 4.20968772e-05]\n",
      " [-3.07627081e-04]\n",
      " [ 5.77930612e-04]\n",
      " [-8.66999541e-04]\n",
      " [-5.30531450e-04]\n",
      " [-1.84113360e-04]\n",
      " [-4.28469722e-04]\n",
      " [ 1.36011838e-04]\n",
      " [-5.44202517e-04]\n",
      " [-1.11053240e-04]\n",
      " [-2.89127514e-04]\n",
      " [ 3.11544081e-04]\n",
      " [ 6.64764016e-06]\n",
      " [-2.40758495e-04]\n",
      " [ 7.91412804e-05]\n",
      " [ 5.88522589e-04]\n",
      " [-4.47396761e-04]\n",
      " [ 3.13300421e-04]\n",
      " [ 2.59824600e-04]\n",
      " [ 1.67387375e-04]\n",
      " [ 1.73983184e-04]\n",
      " [ 2.94934868e-05]\n",
      " [ 1.18644379e-04]\n",
      " [-4.85569959e-04]\n",
      " [ 8.95384606e-04]\n",
      " [-7.57839955e-04]\n",
      " [-2.12165033e-04]\n",
      " [-2.40114578e-04]\n",
      " [-1.34886073e-04]\n",
      " [ 1.99041981e-04]\n",
      " [ 4.41478289e-04]\n",
      " [ 2.14616616e-04]\n",
      " [-3.38868020e-04]\n",
      " [-7.60325187e-04]\n",
      " [ 8.55032489e-04]]\n",
      "[[-1.27679325 -0.96439832  1.04291304 ...  0.75617621 -0.47608385\n",
      "   2.47214373]\n",
      " [ 0.43518585 -0.54729    -0.89701211 ... -3.01542103  1.70477054\n",
      "   1.26048042]\n",
      " [-0.02433672  1.06454574 -0.57590747 ...  0.35364007  0.11638852\n",
      "  -0.52118709]\n",
      " ...\n",
      " [ 1.20823046  0.81841785 -0.30947183 ...  0.02778309  0.42366845\n",
      "   0.57881984]\n",
      " [ 0.33140514 -1.08024513 -0.96934574 ... -0.56782067  0.39570528\n",
      "   0.828497  ]\n",
      " [ 0.75331501 -0.3192933  -0.49222453 ... -0.01347615 -0.765716\n",
      "   0.00939219]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[ 0.09994289  2.16697988 -0.65296107 ...  0.07284606  0.73111404\n",
      "   0.77012653]\n",
      " [-0.25727609  1.63359934  1.21940179 ...  0.80601441 -0.75616645\n",
      "   0.57160253]\n",
      " [ 1.21052092 -0.34703116  0.21082987 ...  0.31667716 -1.77183442\n",
      "   0.39505478]\n",
      " ...\n",
      " [ 1.31961251  0.37341173 -0.17122658 ... -1.08786572 -0.19981561\n",
      "   1.68185183]\n",
      " [ 1.83423484 -1.90941994 -0.49577521 ... -2.09748419 -0.7975918\n",
      "  -0.12986167]\n",
      " [-0.19825007  1.54599124 -0.23719269 ...  0.61568104 -0.96003203\n",
      "  -0.38096334]]\n",
      "[[-0.1051361 ]\n",
      " [ 0.00529524]\n",
      " [ 0.0778148 ]\n",
      " [-0.09575846]\n",
      " [-0.11839615]\n",
      " [ 0.00117551]\n",
      " [ 0.01533179]\n",
      " [ 0.04155244]\n",
      " [ 0.00250843]\n",
      " [-0.00499129]\n",
      " [ 0.0615653 ]\n",
      " [ 0.00777092]\n",
      " [-0.06385747]\n",
      " [-0.0159446 ]\n",
      " [-0.00645844]\n",
      " [-0.00299593]\n",
      " [-0.05347857]\n",
      " [-0.0109546 ]\n",
      " [ 0.00710467]\n",
      " [-0.00752606]\n",
      " [ 0.07165003]\n",
      " [-0.00747795]\n",
      " [-0.03797241]\n",
      " [ 0.01328795]\n",
      " [ 0.00026865]\n",
      " [-0.0226341 ]\n",
      " [ 0.00727426]\n",
      " [ 0.00305018]\n",
      " [ 0.04012454]\n",
      " [-0.03648898]\n",
      " [-0.00685197]\n",
      " [ 0.09065374]\n",
      " [ 0.07765757]\n",
      " [ 0.02372732]\n",
      " [ 0.02722528]\n",
      " [ 0.10013487]\n",
      " [ 0.01841357]\n",
      " [ 0.00214875]\n",
      " [-0.02475839]\n",
      " [-0.10824853]\n",
      " [-0.01398913]\n",
      " [-0.08460416]\n",
      " [-0.00684411]\n",
      " [ 0.02522259]\n",
      " [ 0.03562844]\n",
      " [-0.04912716]\n",
      " [ 0.11654669]\n",
      " [ 0.0246345 ]\n",
      " [-0.01603233]\n",
      " [-0.01864862]\n",
      " [-0.06671073]\n",
      " [-0.00914877]\n",
      " [-0.07731955]\n",
      " [ 0.02202863]\n",
      " [-0.04122217]\n",
      " [-0.00419257]\n",
      " [ 0.02576217]\n",
      " [-0.01429568]\n",
      " [ 0.06473439]\n",
      " [-0.0218851 ]\n",
      " [ 0.03557089]\n",
      " [ 0.006375  ]\n",
      " [ 0.05481152]\n",
      " [-0.0163711 ]\n",
      " [-0.07142568]\n",
      " [-0.01669051]\n",
      " [ 0.02867917]\n",
      " [ 0.07129749]\n",
      " [ 0.05186512]\n",
      " [-0.03546465]\n",
      " [-0.13906395]\n",
      " [-0.00519718]\n",
      " [ 0.00714087]\n",
      " [-0.00223236]\n",
      " [ 0.00036674]\n",
      " [ 0.09219094]\n",
      " [ 0.00220953]\n",
      " [-0.02575292]\n",
      " [ 0.0041932 ]\n",
      " [ 0.05225839]\n",
      " [ 0.00227045]\n",
      " [-0.09585023]\n",
      " [-0.05033562]\n",
      " [-0.07945508]\n",
      " [-0.05862563]\n",
      " [-0.08676682]\n",
      " [ 0.03735688]\n",
      " [-0.01814063]\n",
      " [ 0.05796508]\n",
      " [ 0.04198883]\n",
      " [ 0.07141942]\n",
      " [-0.00706278]\n",
      " [ 0.01844591]\n",
      " [-0.00278489]\n",
      " [-0.00028983]\n",
      " [-0.07713037]\n",
      " [-0.11988031]\n",
      " [-0.0772673 ]\n",
      " [-0.03087813]\n",
      " [-0.06939114]\n",
      " [ 0.00346564]\n",
      " [-0.06686723]\n",
      " [ 0.01925524]\n",
      " [ 0.04648055]\n",
      " [ 0.00684129]\n",
      " [ 0.08363963]\n",
      " [ 0.00290281]\n",
      " [-0.07202434]\n",
      " [-0.00019211]\n",
      " [-0.05921556]\n",
      " [ 0.06866973]\n",
      " [ 0.04455973]\n",
      " [-0.06352719]\n",
      " [ 0.01156357]\n",
      " [-0.04081939]\n",
      " [ 0.01555854]\n",
      " [ 0.11464106]\n",
      " [-0.00528622]\n",
      " [-0.00934695]\n",
      " [ 0.02660986]\n",
      " [ 0.07883472]\n",
      " [ 0.0181576 ]\n",
      " [ 0.065622  ]\n",
      " [ 0.02210431]\n",
      " [-0.00334971]\n",
      " [ 0.03803384]\n",
      " [ 0.00079785]\n",
      " [-0.05929731]\n",
      " [ 0.12684374]]\n",
      "[[-1.79997482 -0.40068025 -0.04667426 ... -0.32871366  1.46316337\n",
      "  -0.39173049]\n",
      " [-0.42283493 -1.27230581 -0.73457957 ...  0.83296194 -0.19327858\n",
      "   1.14416168]\n",
      " [ 0.33250796 -0.21879948 -0.84397632 ...  0.60075492 -1.07249346\n",
      "   1.14773819]\n",
      " ...\n",
      " [ 0.02110039 -2.47425466  0.91370093 ... -0.28483337 -0.05886951\n",
      "   0.04666057]\n",
      " [ 1.07224855  0.93526814  0.64251743 ... -0.98828722 -0.0248701\n",
      "  -1.91586444]\n",
      " [ 0.35551143 -0.53880418  1.11334519 ... -0.73671101  0.26627638\n",
      "   0.18336645]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[-0.26781646  0.11697604  0.72342325 ...  2.26326355  0.16319959\n",
      "  -1.47249413]\n",
      " [ 0.8929251  -0.31180978  0.4962619  ... -0.97238318 -1.39575433\n",
      "   1.64871799]\n",
      " [-1.63290639 -0.22456086 -0.41884507 ...  0.30371971  1.46286491\n",
      "   0.47389891]\n",
      " ...\n",
      " [-0.17999399 -0.16961018  0.03230799 ...  1.57543966 -0.47633461\n",
      "  -1.84931753]\n",
      " [ 2.08114469  0.26126789  1.01698064 ...  1.49806299 -0.7441085\n",
      "   0.64084378]\n",
      " [-0.70277823  0.53994081 -0.71623721 ... -0.19522927 -0.21420859\n",
      "  -1.82410292]]\n",
      "[[-5.97653382e-03]\n",
      " [ 1.98915644e-01]\n",
      " [ 1.78580219e-02]\n",
      " [-1.00207140e-01]\n",
      " [ 4.30418394e-03]\n",
      " [ 3.34726405e-02]\n",
      " [-4.06630612e-02]\n",
      " [ 5.95835376e-05]\n",
      " [ 4.93967123e-03]\n",
      " [ 2.46660803e-02]\n",
      " [ 1.68289852e-03]\n",
      " [ 1.72520668e-03]\n",
      " [-2.34378638e-02]\n",
      " [-1.85142492e-02]\n",
      " [-6.19718396e-02]\n",
      " [-2.58084441e-02]\n",
      " [ 2.94602252e-02]\n",
      " [-5.11649574e-02]\n",
      " [-1.55826893e-03]\n",
      " [-4.14753195e-02]\n",
      " [-3.34668175e-02]\n",
      " [ 6.42154264e-03]\n",
      " [ 1.12296987e-02]\n",
      " [ 1.75727336e-03]\n",
      " [-1.47477703e-01]\n",
      " [ 4.87607652e-03]\n",
      " [ 1.23224197e-02]\n",
      " [ 1.29709589e-01]\n",
      " [ 1.67419065e-03]\n",
      " [-6.64739317e-02]\n",
      " [ 2.11734649e-03]\n",
      " [-1.32353683e-01]\n",
      " [ 1.85435592e-03]\n",
      " [ 1.53924836e-06]\n",
      " [-6.87103914e-03]\n",
      " [-5.31673355e-02]\n",
      " [ 1.79495758e-02]\n",
      " [ 2.47857009e-02]\n",
      " [-2.39878895e-03]\n",
      " [ 2.34477790e-02]\n",
      " [-1.93872286e-02]\n",
      " [ 5.19166102e-02]\n",
      " [ 1.68499004e-03]\n",
      " [ 5.27423994e-03]\n",
      " [ 3.82634120e-02]\n",
      " [ 1.50157606e-02]\n",
      " [ 9.73846835e-02]\n",
      " [ 1.73471878e-02]\n",
      " [ 5.01519623e-05]\n",
      " [ 3.33944296e-02]\n",
      " [ 8.35234245e-02]\n",
      " [-8.66097275e-02]\n",
      " [ 5.70409005e-02]\n",
      " [ 2.05286725e-02]\n",
      " [ 1.60801150e-03]\n",
      " [-3.55274510e-02]\n",
      " [ 6.38571033e-05]\n",
      " [-1.80465576e-02]\n",
      " [-8.05763511e-03]\n",
      " [-8.88037235e-04]\n",
      " [-1.41007241e-03]\n",
      " [-7.49705139e-02]\n",
      " [ 1.14058818e-01]\n",
      " [-6.22307038e-04]\n",
      " [ 2.23010389e-02]\n",
      " [ 5.74001598e-02]\n",
      " [ 6.77993749e-03]\n",
      " [-3.26858143e-02]\n",
      " [ 7.17421091e-03]\n",
      " [-1.62555056e-02]\n",
      " [ 4.67313735e-02]\n",
      " [-2.08018223e-03]\n",
      " [-4.19423305e-02]\n",
      " [ 9.12500761e-02]\n",
      " [-2.78842683e-02]\n",
      " [ 2.74657367e-02]\n",
      " [ 3.40832358e-03]\n",
      " [ 8.31692213e-03]\n",
      " [-2.66913867e-05]\n",
      " [ 5.54227897e-02]\n",
      " [-4.63293265e-02]\n",
      " [-1.79010503e-02]\n",
      " [ 1.40196362e-02]\n",
      " [-1.33313811e-03]\n",
      " [ 4.23667523e-03]\n",
      " [ 1.44708718e-05]\n",
      " [ 8.06790882e-03]\n",
      " [-8.34468494e-04]\n",
      " [ 4.08309151e-03]\n",
      " [-2.67993098e-02]\n",
      " [ 7.20680267e-03]\n",
      " [-6.90674941e-02]\n",
      " [ 1.57218565e-02]\n",
      " [ 6.64927488e-04]\n",
      " [ 7.62886150e-03]\n",
      " [-9.12331410e-03]\n",
      " [ 8.83202952e-02]\n",
      " [-1.49946473e-02]\n",
      " [ 8.44970407e-02]\n",
      " [ 4.66957232e-02]\n",
      " [-1.02640297e-01]\n",
      " [ 7.36345939e-03]\n",
      " [-9.86197539e-04]\n",
      " [-5.88926702e-02]\n",
      " [-4.29535055e-04]\n",
      " [ 6.44332832e-02]\n",
      " [-1.36309136e-02]\n",
      " [-6.11928177e-02]\n",
      " [-1.06312705e-01]\n",
      " [ 1.48820939e-01]\n",
      " [ 9.63087998e-03]\n",
      " [ 1.87991587e-02]\n",
      " [-5.34261620e-02]\n",
      " [ 3.20786502e-02]\n",
      " [ 2.68037815e-02]\n",
      " [ 2.67169783e-02]\n",
      " [-8.25706211e-02]\n",
      " [ 3.02114425e-02]\n",
      " [-5.66150929e-03]\n",
      " [ 2.85339114e-02]\n",
      " [ 1.15366165e-01]\n",
      " [-9.40439893e-02]\n",
      " [ 1.03467084e-03]\n",
      " [-2.79257500e-02]\n",
      " [-2.96639748e-02]\n",
      " [-6.03330596e-02]\n",
      " [-7.50530812e-02]\n",
      " [-3.01810257e-02]\n",
      " [ 1.68194553e-01]\n",
      " [-9.13186165e-03]]\n",
      "[[ 0.1730057   0.00756205  0.28739791 ... -1.56523721  1.97766355\n",
      "  -0.97877754]\n",
      " [-0.8110167   0.84748017 -0.49546112 ...  0.37984802 -0.51294114\n",
      "   0.95649959]\n",
      " [ 0.33193746 -0.80037267  0.47333796 ... -1.85520564 -0.08020005\n",
      "  -0.01744692]\n",
      " ...\n",
      " [-0.28157819  0.86807151 -0.49506917 ... -0.89091393  0.22733727\n",
      "  -1.43838934]\n",
      " [-1.42737664 -0.17023288 -1.31432345 ...  1.09139579  0.37735376\n",
      "  -0.10917141]\n",
      " [-0.53700365 -1.36085511 -0.03391217 ...  0.21816934  0.08592562\n",
      "  -0.24499011]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[-0.17289524  0.89916998 -0.77334595 ... -1.73762259  0.0877747\n",
      "  -0.15590131]\n",
      " [ 0.17405617 -1.22460302  0.51732756 ...  0.21962595 -1.26293413\n",
      "  -0.08218562]\n",
      " [-0.41724536  1.67649575 -0.00468737 ... -0.48357012  1.29806534\n",
      "   0.44707402]\n",
      " ...\n",
      " [-0.29367433 -1.42801305 -0.27615012 ...  1.30687956 -0.67355832\n",
      "  -0.0302912 ]\n",
      " [-1.87623484 -1.87239627 -0.18077405 ...  0.16195786 -0.34637931\n",
      "  -0.17860911]\n",
      " [ 1.02865637  1.50757272 -0.19284231 ... -1.39900106  0.74740323\n",
      "   0.50252073]]\n",
      "[[ 0.00351478]\n",
      " [-0.18396014]\n",
      " [ 0.08527162]\n",
      " [-0.02661525]\n",
      " [-0.02831706]\n",
      " [ 0.09581289]\n",
      " [ 0.10750454]\n",
      " [ 0.04247563]\n",
      " [-0.04237955]\n",
      " [-0.05330744]]\n",
      "[[ 0.83983189  0.13617594  0.034706   ...  0.05739004 -0.46395821\n",
      "   1.84679212]\n",
      " [-0.63453185  0.45997961  0.19259413 ...  0.48219711 -0.23284741\n",
      "  -0.1389119 ]\n",
      " [ 0.09150276  1.15797884 -1.70816183 ... -0.85883239 -0.04384018\n",
      "  -0.14862884]\n",
      " ...\n",
      " [ 1.94641753  0.44011791 -0.16699793 ... -0.16200471  0.1040101\n",
      "  -1.46287577]\n",
      " [-0.30063685 -0.63274184  1.66617818 ... -0.43275534  1.06597132\n",
      "  -1.12622021]\n",
      " [-1.61806984 -2.33296274 -0.44155966 ...  0.69218912  0.04149022\n",
      "   0.26481686]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_FEATURES = X_train.shape[0]\n",
    "WEIGHT_INITIALIZER = \"random\"\n",
    "NUM_HIDDEN_LAYERS = 3\n",
    "HIDDEN_LAYER_DIMS = (128+np.arange(NUM_HIDDEN_LAYERS)).astype(int)\n",
    "# HIDDEN_LAYER_DIMS = (128+np.zeros(NUM_HIDDEN_LAYERS)).astype(int)\n",
    "OPTIMIZER = \"sgd\"\n",
    "LEARNING_RATE = 0.001\n",
    "ACTIVATION = \"sigmoid\"\n",
    "OUTPUT_LAYER_DIM = num_classes\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1\n",
    "WEIGHT_DECAY = 0\n",
    "\n",
    "N = NeuralNet(num_features = NUM_FEATURES,\n",
    "                weight_initializer = WEIGHT_INITIALIZER,\n",
    "                num_hidden_layers = NUM_HIDDEN_LAYERS,\n",
    "                hidden_layer_dims = HIDDEN_LAYER_DIMS,\n",
    "                optimizer = OPTIMIZER,\n",
    "                learning_rate = LEARNING_RATE,\n",
    "                activation = ACTIVATION,\n",
    "                X_train = X_train,\n",
    "                Y_train = Y_train,\n",
    "                Xv = Xv,\n",
    "                Yv = Yv,\n",
    "                weight_decay=WEIGHT_DECAY,\n",
    "                output_layer_dim = OUTPUT_LAYER_DIM,\n",
    "                batch_size = BATCH_SIZE,\n",
    "                num_epochs = EPOCHS)\n",
    "\n",
    "# N.initialize_hidden_layers()\n",
    "# N.initialize_NeuralNet()\n",
    "# N.ForwardPropagation(N.X_train[:,0:500])\n",
    "# print(N.Y_train[:,0:500].shape)\n",
    "# print(N.output_layer.h.shape)\n",
    "# N.BackPropagation(N.output_layer.h,N.Y_train[:,0:500],N.X_train[:,0:500])\n",
    "# N.print_all()\n",
    "N.fit_NeuralNet()\n",
    "N.print_all()\n",
    "# print(X_train.shape)\n",
    "# for i in range(X_train.shape[0]):\n",
    "#     N.ForwardPropagation(X_train[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 54000)\n",
      "(10, 54000)\n"
     ]
    }
   ],
   "source": [
    "# print(N.activation(0))\n",
    "print(N.X_train.shape)\n",
    "print(N.Y_train.shape)\n",
    "# for i in range(0,N.X_train.shape[1],N.batch_size):\n",
    "#     curr_batch = min(N.X_train.shape[1]-i,N.batch_size)\n",
    "#     print(i,i+curr_batch)\n",
    "# ty = np.arange(10)\n",
    "# print(ty[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4262/755351156.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1. /(1. + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_pred_train = N.predict_NeuralNet(N.X_train)\n",
    "# print(accuracy_score(y_pred=Y_pred_train,y_true=Y_train))\n",
    "# for i in Y_pred_train:\n",
    "#     print(i)\n",
    "# for i in Y_pred_train:\n",
    "#     print(i)\n",
    "# print(Y_pred_train)\n",
    "# print(accuracy_score(y_pred=Y_pred_train,y_true=Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6001296296296297\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred=Y_pred_train,y_true=Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# a=np.array([[1,2,3],[1,2,3]])\n",
    "b = np.zeros(100)\n",
    "a = np.array([])\n",
    "a=np.concatenate((a,b))\n",
    "print(np.concatenate((a,b)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n",
      "[9 8 7 6 5 4 3 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# a = np.zeros(shape=(4,3)) + 3\n",
    "print(Y_train.shape)\n",
    "# b = np.zeros(shape=(3,10)) + 4\n",
    "\n",
    "# c = np.zeros(4).reshape(4,1) + 1\n",
    "# # print(c)\n",
    "# x = np.matmul(a,b) + c\n",
    "# print(x)\n",
    "# print(sigmoid(a))\n",
    "# print(b)\n",
    "# print(a*b)\n",
    "# l = []\n",
    "# for i in range(0,50,10):\n",
    "#     l.append(min(50-i,10))\n",
    "# print(l)\n",
    "# ty =np.zeros((2,))\n",
    "# dy =np.zeros((2,))\n",
    "# print(ty.shape)\n",
    "# ty.reshape(1,2)\n",
    "# print(np.matmul(np.transpose(ty),dy))\n",
    "# t = Y_train[0:100].shape\n",
    "# print(t)\n",
    "# a = np.zeros((10,10))\n",
    "# b = np.zeros((5,3))+12\n",
    "# b.resize((5,5))\n",
    "# print(b)\n",
    "# print(X_test.shape)\n",
    "print(np.flip(np.arange(10)))\n",
    "# a[:],b = b,a\n",
    "# print((128+np.zeros(NUM_HIDDEN_LAYERS)).astype(int))\n",
    "# print(b[0:2])\n",
    "# print(np.flip(b))\n",
    "# x = jk.reshape((jk.shape[0],10))\n",
    "# print(jk.reshape((10,1)))\n",
    "# print(X_train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
